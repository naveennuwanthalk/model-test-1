{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Setup Authentication & Libraries"
      ],
      "metadata": {
        "id": "I4M-IC5j8rJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers datasets huggingface_hub requests\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import requests\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import HfApi, Repository\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "\n",
        "# Verify GPU availability\n",
        "print(f\"GPU Available: {torch.cuda.is_available()}\")"
      ],
      "metadata": {
        "id": "nR9-rl1C8sQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Configuration & Secrets Setup\n"
      ],
      "metadata": {
        "id": "jvRTN3C_8vzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== EDIT THESE VALUES ======\n",
        "GITHUB_REPO = \"naveennuwanthalk/model-test-1\"  # e.g. \"johnsmith/llm-checkpoints\"\n",
        "MODEL_NAME = \"nuxara-small\"  # Start with small model for Colab compatibility\n",
        "CHECKPOINT_INTERVAL = 300  # Save every 300 steps\n",
        "# ===============================\n",
        "\n",
        "# Set up secrets (Add these in Colab's üîë Secrets manager)\n",
        "OPENROUTER_API_KEY = userdata.get('OPENROUTER_API_KEY')\n",
        "GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')"
      ],
      "metadata": {
        "id": "2iymuLRh8xJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Initialize GitHub Repository\n"
      ],
      "metadata": {
        "id": "fxnBlmEI8yZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone/pull the repo\n",
        "repo = Repository(\n",
        "    local_dir=\"checkpoints\",\n",
        "    clone_from=f\"https://{GITHUB_TOKEN}@github.com/{GITHUB_REPO}.git\",\n",
        "    use_auth_token=GITHUB_TOKEN\n",
        ")\n",
        "\n",
        "# Create necessary directories\n",
        "os.makedirs(\"checkpoints/model\", exist_ok=True)"
      ],
      "metadata": {
        "id": "vRS-Rtxu8zRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. DeepSeek-R1 Data Generation\n"
      ],
      "metadata": {
        "id": "xEz5Xk-R80wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_training_data(prompt):\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"HTTP-Referer\": \"https://github.com\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    response = requests.post(\n",
        "        \"https://openrouter.ai/api/v1/chat/completions\",\n",
        "        headers=headers,\n",
        "        json={\n",
        "            \"model\": \"deepseek-ai/deepseek-r1\",\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "            \"temperature\": 0.7\n",
        "        }\n",
        "    )\n",
        "\n",
        "    return response.json()['choices'][0]['message']['content'] if response.status_code == 200 else None\n",
        "\n",
        "# Generate dataset\n",
        "prompts = [\n",
        "    \"Explain machine learning to a 5-year-old:\",\n",
        "    \"Write Python code to calculate Fibonacci sequence:\",\n",
        "    \"What is the capital of Japan?\",\n",
        "]\n",
        "\n",
        "dataset = []\n",
        "for prompt in prompts:\n",
        "    completion = generate_training_data(prompt)\n",
        "    if completion:\n",
        "        dataset.append({\"text\": f\"{prompt}\\n{completion}\"})\n",
        "\n",
        "print(f\"Generated {len(dataset)} training examples\")"
      ],
      "metadata": {
        "id": "XUTnkjyy81ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Model & Tokenizer Initialization\n"
      ],
      "metadata": {
        "id": "P-zvRuQl83Jd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Prepare dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_dataset = [tokenize_function(ex) for ex in dataset]"
      ],
      "metadata": {
        "id": "2Sdo11pv83_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Checkpoint Management System\n"
      ],
      "metadata": {
        "id": "p0ME-hr-85Lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CheckpointManager:\n",
        "    def save_checkpoint(self, model, step):\n",
        "        # Save model files\n",
        "        model.save_pretrained(\"checkpoints/model\")\n",
        "        tokenizer.save_pretrained(\"checkpoints/model\")\n",
        "\n",
        "        # Save training state\n",
        "        torch.save({\n",
        "            'step': step,\n",
        "            'optimizer_state': trainer.optimizer.state_dict(),\n",
        "        }, \"checkpoints/training_state.pt\")\n",
        "\n",
        "        # Commit to GitHub\n",
        "        repo.git_add(auto_lfs_track=True)\n",
        "        repo.git_commit(f\"Checkpoint at step {step}\")\n",
        "        repo.git_push()\n",
        "        print(f\"‚úÖ Checkpoint saved at step {step}\")\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        if os.path.exists(\"checkpoints/training_state.pt\"):\n",
        "            state = torch.load(\"checkpoints/training_state.pt\")\n",
        "            model = AutoModelForCausalLM.from_pretrained(\"checkpoints/model\")\n",
        "            return model, state['step']\n",
        "        return None, 0\n",
        "\n",
        "checkpoint_manager = CheckpointManager()"
      ],
      "metadata": {
        "id": "aqE5BEwA854g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Training Setup with Resume Capability\n"
      ],
      "metadata": {
        "id": "kUPCgmcs87NB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load existing checkpoint if available\n",
        "model, start_step = checkpoint_manager.load_checkpoint()\n",
        "\n",
        "# Configure training\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=2,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=50,\n",
        "    learning_rate=2e-5,\n",
        "    save_strategy=\"no\",  # We handle saving manually\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "735JLoGJ88Ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Custom Training Loop with Auto-Save\n"
      ],
      "metadata": {
        "id": "QPypGdc-89Yg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified training loop\n",
        "for step in range(start_step, trainer.state.max_steps):\n",
        "    trainer.train()\n",
        "\n",
        "    # Save checkpoint at intervals\n",
        "    if step % CHECKPOINT_INTERVAL == 0:\n",
        "        checkpoint_manager.save_checkpoint(trainer.model, step)\n",
        "\n",
        "    # Prevent Colab timeout (keep-alive)\n",
        "    if step % 50 == 0:\n",
        "        print(f\"Step {step}/{trainer.state.max_steps} completed\")\n",
        "\n",
        "# Final save\n",
        "checkpoint_manager.save_checkpoint(trainer.model, step)\n",
        "print(\"üèÅ Training completed successfully!\")"
      ],
      "metadata": {
        "id": "FCLzGUgq8-Th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Recovery & Continuation Instructions\n"
      ],
      "metadata": {
        "id": "zrOekyme8_cx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If Colab disconnects, simply:\n",
        "# 1. Reconnect\n",
        "# 2. Run Sections 1-3 and 5-7\n",
        "# 3. Skip Section 4 (data generation)\n",
        "# 4. Run Section 8 again to resume"
      ],
      "metadata": {
        "id": "FJhL8o-V9Af_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}